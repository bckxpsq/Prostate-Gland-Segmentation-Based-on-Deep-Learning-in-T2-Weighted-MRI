{"cells":[{"cell_type":"markdown","metadata":{"id":"sh6KdaoQIhtR"},"source":["## **Elaborazione di Immagini Mediche**\n","### Contest 2021/22 - Segmentazione ghiandola prostatica in immagini MRI\n"]},{"cell_type":"markdown","metadata":{"id":"j0DGsHkaI3jT"},"source":["\n","\n","*   Collegamento a Google Drive e import delle librerie necessarie\n","\n"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":113675,"status":"ok","timestamp":1639499669593,"user":{"displayName":"Edward Hyde","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00713850021598258581"},"user_tz":-60},"id":"uZD1JvhVfQnI","outputId":"f7b4a106-4ea5-4f2c-ce8d-23056cdb612b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting tensorflow==2.1.0\n","  Downloading tensorflow-2.1.0-cp37-cp37m-manylinux2010_x86_64.whl (421.8 MB)\n","\u001b[K     |████████████████████████████████| 421.8 MB 25 kB/s \n","\u001b[?25hRequirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.1.0) (1.1.2)\n","Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.1.0) (0.12.0)\n","Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.1.0) (3.17.3)\n","Collecting tensorflow-estimator<2.2.0,>=2.1.0rc0\n","  Downloading tensorflow_estimator-2.1.0-py2.py3-none-any.whl (448 kB)\n","\u001b[K     |████████████████████████████████| 448 kB 65.6 MB/s \n","\u001b[?25hRequirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.1.0) (0.2.0)\n","Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.1.0) (1.13.3)\n","Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.1.0) (1.19.5)\n","Collecting tensorboard<2.2.0,>=2.1.0\n","  Downloading tensorboard-2.1.1-py3-none-any.whl (3.8 MB)\n","\u001b[K     |████████████████████████████████| 3.8 MB 39.4 MB/s \n","\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.1.0) (1.1.0)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.1.0) (1.15.0)\n","Collecting gast==0.2.2\n","  Downloading gast-0.2.2.tar.gz (10 kB)\n","Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.1.0) (1.42.0)\n","Requirement already satisfied: scipy==1.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.1.0) (1.4.1)\n","Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.1.0) (0.8.1)\n","Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.1.0) (0.37.0)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.1.0) (3.3.0)\n","Collecting keras-applications>=1.0.8\n","  Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n","\u001b[K     |████████████████████████████████| 50 kB 5.8 MB/s \n","\u001b[?25hRequirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.8->tensorflow==2.1.0) (3.1.0)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (0.4.6)\n","Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (1.35.0)\n","Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (57.4.0)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (2.23.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (3.3.6)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (1.0.1)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (0.2.8)\n","Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (4.2.4)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (4.8)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (1.3.0)\n","Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (4.8.2)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (3.6.0)\n","Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (3.10.0.2)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (0.4.8)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (2021.10.8)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (3.1.1)\n","Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->keras-applications>=1.0.8->tensorflow==2.1.0) (1.5.2)\n","Building wheels for collected packages: gast\n","  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for gast: filename=gast-0.2.2-py3-none-any.whl size=7554 sha256=dc3153ddf45c3fdc4b948337d758eaae577c2d8736dd12b25182b25005397b86\n","  Stored in directory: /root/.cache/pip/wheels/21/7f/02/420f32a803f7d0967b48dd823da3f558c5166991bfd204eef3\n","Successfully built gast\n","Installing collected packages: tensorflow-estimator, tensorboard, keras-applications, gast, tensorflow\n","  Attempting uninstall: tensorflow-estimator\n","    Found existing installation: tensorflow-estimator 2.7.0\n","    Uninstalling tensorflow-estimator-2.7.0:\n","      Successfully uninstalled tensorflow-estimator-2.7.0\n","  Attempting uninstall: tensorboard\n","    Found existing installation: tensorboard 2.7.0\n","    Uninstalling tensorboard-2.7.0:\n","      Successfully uninstalled tensorboard-2.7.0\n","  Attempting uninstall: gast\n","    Found existing installation: gast 0.4.0\n","    Uninstalling gast-0.4.0:\n","      Successfully uninstalled gast-0.4.0\n","  Attempting uninstall: tensorflow\n","    Found existing installation: tensorflow 2.7.0\n","    Uninstalling tensorflow-2.7.0:\n","      Successfully uninstalled tensorflow-2.7.0\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","tensorflow-probability 0.15.0 requires gast>=0.3.2, but you have gast 0.2.2 which is incompatible.\u001b[0m\n","Successfully installed gast-0.2.2 keras-applications-1.0.8 tensorboard-2.1.1 tensorflow-2.1.0 tensorflow-estimator-2.1.0\n","Collecting keras==2.3.1\n","  Downloading Keras-2.3.1-py2.py3-none-any.whl (377 kB)\n","\u001b[K     |████████████████████████████████| 377 kB 5.1 MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.7/dist-packages (from keras==2.3.1) (1.19.5)\n","Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.7/dist-packages (from keras==2.3.1) (1.4.1)\n","Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from keras==2.3.1) (1.1.2)\n","Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from keras==2.3.1) (1.15.0)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from keras==2.3.1) (3.13)\n","Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.7/dist-packages (from keras==2.3.1) (1.0.8)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras==2.3.1) (3.1.0)\n","Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->keras==2.3.1) (1.5.2)\n","Installing collected packages: keras\n","  Attempting uninstall: keras\n","    Found existing installation: keras 2.7.0\n","    Uninstalling keras-2.7.0:\n","      Successfully uninstalled keras-2.7.0\n","Successfully installed keras-2.3.1\n","Collecting segmentation_models==1.0.1\n","  Downloading segmentation_models-1.0.1-py3-none-any.whl (33 kB)\n","Collecting image-classifiers==1.0.0\n","  Downloading image_classifiers-1.0.0-py3-none-any.whl (19 kB)\n","Requirement already satisfied: keras-applications<=1.0.8,>=1.0.7 in /usr/local/lib/python3.7/dist-packages (from segmentation_models==1.0.1) (1.0.8)\n","Collecting efficientnet==1.0.0\n","  Downloading efficientnet-1.0.0-py3-none-any.whl (17 kB)\n","Requirement already satisfied: scikit-image in /usr/local/lib/python3.7/dist-packages (from efficientnet==1.0.0->segmentation_models==1.0.1) (0.18.3)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications<=1.0.8,>=1.0.7->segmentation_models==1.0.1) (3.1.0)\n","Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.7/dist-packages (from keras-applications<=1.0.8,>=1.0.7->segmentation_models==1.0.1) (1.19.5)\n","Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->keras-applications<=1.0.8,>=1.0.7->segmentation_models==1.0.1) (1.5.2)\n","Requirement already satisfied: matplotlib!=3.0.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image->efficientnet==1.0.0->segmentation_models==1.0.1) (3.2.2)\n","Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image->efficientnet==1.0.0->segmentation_models==1.0.1) (2.4.1)\n","Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.7/dist-packages (from scikit-image->efficientnet==1.0.0->segmentation_models==1.0.1) (2021.11.2)\n","Requirement already satisfied: scipy>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from scikit-image->efficientnet==1.0.0->segmentation_models==1.0.1) (1.4.1)\n","Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image->efficientnet==1.0.0->segmentation_models==1.0.1) (2.6.3)\n","Requirement already satisfied: pillow!=7.1.0,!=7.1.1,>=4.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image->efficientnet==1.0.0->segmentation_models==1.0.1) (7.1.2)\n","Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from scikit-image->efficientnet==1.0.0->segmentation_models==1.0.1) (1.2.0)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet==1.0.0->segmentation_models==1.0.1) (3.0.6)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet==1.0.0->segmentation_models==1.0.1) (1.3.2)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet==1.0.0->segmentation_models==1.0.1) (2.8.2)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet==1.0.0->segmentation_models==1.0.1) (0.11.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet==1.0.0->segmentation_models==1.0.1) (1.15.0)\n","Installing collected packages: image-classifiers, efficientnet, segmentation-models\n","Successfully installed efficientnet-1.0.0 image-classifiers-1.0.0 segmentation-models-1.0.1\n","Collecting h5py==2.10.0\n","  Downloading h5py-2.10.0-cp37-cp37m-manylinux1_x86_64.whl (2.9 MB)\n","\u001b[K     |████████████████████████████████| 2.9 MB 5.1 MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.7/dist-packages (from h5py==2.10.0) (1.19.5)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from h5py==2.10.0) (1.15.0)\n","Installing collected packages: h5py\n","  Attempting uninstall: h5py\n","    Found existing installation: h5py 3.1.0\n","    Uninstalling h5py-3.1.0:\n","      Successfully uninstalled h5py-3.1.0\n","Successfully installed h5py-2.10.0\n","Collecting plotly==5.3.1\n","  Downloading plotly-5.3.1-py2.py3-none-any.whl (23.9 MB)\n","\u001b[K     |████████████████████████████████| 23.9 MB 77 kB/s \n","\u001b[?25hCollecting tenacity>=6.2.0\n","  Downloading tenacity-8.0.1-py3-none-any.whl (24 kB)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from plotly==5.3.1) (1.15.0)\n","Installing collected packages: tenacity, plotly\n","  Attempting uninstall: plotly\n","    Found existing installation: plotly 4.4.1\n","    Uninstalling plotly-4.4.1:\n","      Successfully uninstalled plotly-4.4.1\n","Successfully installed plotly-5.3.1 tenacity-8.0.1\n"]}],"source":["# Before running the script, reset the runtime to factory reset (Runtime -> Factory Reset Runtime)\n","# and then change runtime type to GPU (Runtime -> Change runtime type)\n","\n","# Install libriary dependencies for running deep learning\n","!pip install tensorflow==2.1.0\n","!pip install keras==2.3.1\n","!pip install segmentation_models==1.0.1\n","!pip install h5py==2.10.0 "]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":20474,"status":"ok","timestamp":1639499736917,"user":{"displayName":"Edward Hyde","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00713850021598258581"},"user_tz":-60},"id":"OpJAl9SpUB8D","outputId":"77fcf9fa-6fc7-49c4-8ec3-f2c4092fd662"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]},{"output_type":"stream","name":"stderr","text":["Using TensorFlow backend.\n"]},{"output_type":"stream","name":"stdout","text":["Segmentation Models: using `keras` framework.\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import os\n","import random\n","import numpy as np\n","import plotly.express as px\n","\n","from matplotlib import pyplot as plt\n","from tqdm import tqdm\n","from skimage.io import imread, imshow, imsave\n","from skimage.transform import resize\n","from skimage.segmentation import mark_boundaries\n","from scipy import ndimage\n","from skimage.util import crop\n","from skimage.morphology import binary_dilation\n","\n","from keras.callbacks import ModelCheckpoint\n","from keras.callbacks import CSVLogger\n","from keras.callbacks import EarlyStopping\n","from keras.utils.np_utils import to_categorical\n","from keras.preprocessing.image import ImageDataGenerator\n","from keras.models import load_model\n","\n","from segmentation_models import Unet"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3562,"status":"ok","timestamp":1639500576001,"user":{"displayName":"Edward Hyde","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00713850021598258581"},"user_tz":-60},"id":"C9vABYz1bM5l","outputId":"97c53014-de8d-4192-e980-e9a4f806b8bb"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: unrar in /usr/local/lib/python3.7/dist-packages (0.4)\n","\n","UNRAR 5.50 freeware      Copyright (c) 1993-2017 Alexander Roshal\n","\n","Cannot open DATASET_stu.rar\n","No such file or directory\n","No files to extract\n"]}],"source":["# LOADING DATASET FROM DRIVE TO COLAB\n","\n","!pip install unrar\n","!unrar x \"drive/MyDrive/gruppo_FA_DO_PA_PA/DATASET_stu.rar\"   # inserire path completa in cui si trovano i dati (es. \"drive/MyDrive/..../DATASET_stu.rar\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":31054,"status":"ok","timestamp":1639482568958,"user":{"displayName":"Kaçrub Angelo Pasqua","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06238770462519031866"},"user_tz":-60},"id":"WbWFShQOvfIm","outputId":"7b2cd7f4-fcb6-437c-b771-c104316f5314"},"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 32/32 [00:08<00:00,  3.65it/s]\n","100%|██████████| 8/8 [00:02<00:00,  3.89it/s]\n"]}],"source":["#PREPROCESSING AND STORING IMAGES OF TRAINING AND VALIDATION\n","\n","\n","dataset_name = 'DATASET_stu'\n","\n","# Paths\n","TRAIN_IMG_path = os.path.join(dataset_name,'train','images')\n","TRAIN_MASK_path = os.path.join(dataset_name,'train','manual')\n","VAL_IMG_path = os.path.join(dataset_name,'val','images')\n","VAL_MASK_path = os.path.join(dataset_name,'val','manual')\n","\n","#Extracting list of volumes of training e validation set\n","train_images = os.listdir(TRAIN_IMG_path)\n","val_images = os.listdir(VAL_IMG_path)\n","\n","# pre_process and store\n","IMG_WIDTH = 256     # we will crop images to 256x256\n","IMG_HEIGHT = 256\n","IMG_CHANNELS = 3\n","NUM_CLASSES = 2\n","\n","\n","\n","# load training set \n","for n, id_ in tqdm(enumerate(train_images), total=len(train_images)):\n","\n","    vol = imread(TRAIN_IMG_path+'/'+id_)  #reading current RM volume\n","    vol = crop(vol,((0,0),(128,128),(100,156)),copy=False)  #cropping slices of volume\n","    \n","    for sl in range(vol.shape[0]):\n","       vol[sl,:,:] = ndimage.median_filter(vol[sl,:,:],3)  #filtering each slice of the volume w/ median filter of kernel dimension 3x3\n","\n","    mask = imread(TRAIN_MASK_path+'/'+id_)  #reading respective mask\n","    mask = crop(mask,((0,0),(128,128),(100,156)),copy=False)  #cropping masks\n","\n","    if n == 0:\n","        X_train_temp = np.copy(vol)\n","        Y_train_temp = np.copy(mask)\n","    else:\n","        X_train_temp = np.append(X_train_temp,vol,axis=0)\n","        Y_train_temp = np.append(Y_train_temp,mask,axis=0)\n","\n","# creating array which will indicate how to augment data of our training set (slices at border of prostate will be statically\n","# augmented of a factor 4 w/ respect to other slices)\n","aug = np.any(Y_train_temp,axis=(1,2))   #array indicating slices on which prostate has been manually detected \n","aug = ndimage.distance_transform_edt(binary_dilation(aug,selem=np.ones((3),dtype=bool)))  # we want to augment of factor 4 also first \n","                                                                                          # slices w/ no prostate  \n","aug[np.logical_and(aug >0, aug < 5)] = 4  # slice immediately before and after prostate and first 3 slices at each border will be augmented\n","aug[aug != 4] = 1\n","aug = aug.astype(np.uint8)\n","\n","# statically augmenting X_train and Y_train\n","X_train = np.empty((0,IMG_HEIGHT,IMG_WIDTH),dtype=np.uint8)\n","Y_train = np.empty((0,IMG_HEIGHT,IMG_WIDTH),dtype=np.uint8)\n","for i in range(len(aug)):\n","    currX = X_train_temp[i]\n","    currX = np.broadcast_to(currX,(aug[i],X_train_temp.shape[1],X_train_temp.shape[2]))\n","    currY = Y_train_temp[i]\n","    currY = np.broadcast_to(currY,(aug[i],X_train_temp.shape[1],X_train_temp.shape[2]))\n","    X_train = np.append(X_train,currX,axis=0)\n","    Y_train = np.append(Y_train,currY,axis=0)\n","\n","X_train = np.stack([X_train,X_train,X_train],axis=3)  #conversion of stored volumes from grayscale to rgb\n","Y_train = to_categorical(Y_train, num_classes=NUM_CLASSES, dtype='float32')\n","\n","\n","\n","# load validation set\n","for n, id_ in tqdm(enumerate(val_images), total=len(val_images)):\n","\n","    vol = imread(VAL_IMG_path+'/'+id_)\n","    vol = crop(vol,((0,0),(128,128),(100,156)),copy=False) #cropping images\n","    for sl in range(vol.shape[0]):\n","       vol[sl,:,:] = ndimage.median_filter(vol[sl,:,:],3)\n","\n","    mask = imread(VAL_MASK_path+'/'+id_)\n","    mask = crop(mask,((0,0),(128,128),(100,156)),copy=False) #cropping\n","\n","    if n == 0:\n","        X_val = np.copy(vol)\n","        Y_val = np.copy(mask)\n","    else:\n","        X_val = np.append(X_val,vol,axis=0)\n","        Y_val = np.append(Y_val,mask,axis=0)\n","\n","X_val = np.stack([X_val,X_val,X_val],axis=3)  #conversion of stored volumes from grayscale to rgb\n","Y_val = to_categorical(Y_val, num_classes=NUM_CLASSES, dtype='float32')"]},{"cell_type":"code","source":["#DATA AUGMENTATION\n","\n","# Data augmentation (training set)\n","image_datagen = ImageDataGenerator(rotation_range = 5,\n","                                   zoom_range = [1,1.25]\n","                                   )\n","\n","# Data augmentation (validation set)\n","val_datagen = ImageDataGenerator()\n","\n","# Generator \n","def XYaugmentGenerator(X1, y, batch_sz, sd):\n","        genX1 = image_datagen.flow(X1, y, batch_size=batch_sz,seed = sd)\n","        genX2 = image_datagen.flow(y, X1, batch_size=batch_sz, seed = sd)\n","        while True:\n","            X1i = genX1.next()  \n","            X2i = genX2.next()\n","            yield X1i[0], X2i[0]"],"metadata":{"id":"9MOWSndpSvIh"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IoIfU6PZpQ3M","outputId":"e3a606c5-a6a6-46f8-a940-c996884a23f8","executionInfo":{"status":"ok","timestamp":1639487304517,"user_tz":-60,"elapsed":2531049,"user":{"displayName":"Kaçrub Angelo Pasqua","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06238770462519031866"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/20\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/keras_preprocessing/image/numpy_array_iterator.py:136: UserWarning: NumpyArrayIterator is set to use the data format convention \"channels_last\" (channels on axis 3), i.e. expected either 1, 3, or 4 channels on axis 3. However, it was passed an array with shape (1536, 256, 256, 2) (2 channels).\n","  str(self.x.shape[channels_axis]) + ' channels).')\n"]},{"output_type":"stream","name":"stdout","text":["768/768 [==============================] - 479s 623ms/step - loss: 0.1167 - binary_accuracy: 0.9557 - val_loss: 0.1077 - val_binary_accuracy: 0.9427\n","Epoch 2/20\n","768/768 [==============================] - 392s 510ms/step - loss: 0.0610 - binary_accuracy: 0.9737 - val_loss: 0.1134 - val_binary_accuracy: 0.9567\n","Epoch 3/20\n","768/768 [==============================] - 391s 509ms/step - loss: 0.0436 - binary_accuracy: 0.9804 - val_loss: 0.0295 - val_binary_accuracy: 0.9817\n","Epoch 4/20\n","768/768 [==============================] - 391s 509ms/step - loss: 0.0412 - binary_accuracy: 0.9810 - val_loss: 0.0420 - val_binary_accuracy: 0.9756\n","Epoch 5/20\n","768/768 [==============================] - 391s 509ms/step - loss: 0.0352 - binary_accuracy: 0.9836 - val_loss: 0.0242 - val_binary_accuracy: 0.9818\n","Epoch 6/20\n","768/768 [==============================] - 390s 508ms/step - loss: 0.0272 - binary_accuracy: 0.9867 - val_loss: 0.4340 - val_binary_accuracy: 0.9790\n","Restoring model weights from the end of the best epoch\n","Epoch 00006: early stopping\n"]}],"source":["# NET DEFINITION AND TRAINING\n","\n","# UNET adopted net definition\n","BACKBONE = 'densenet201'\n","model = Unet(backbone_name = BACKBONE,\n","            input_shape = (IMG_HEIGHT,IMG_WIDTH,IMG_CHANNELS),\n","            encoder_weights = 'imagenet', \n","            encoder_freeze = True,\n","            decoder_block_type = 'transpose',\n","            classes = NUM_CLASSES,\n","            activation = 'sigmoid')\n","\n","# definition of optimization algorithm and loss function adopted \n","model.compile('Adam', loss='binary_crossentropy', metrics=['binary_accuracy'])\n","\n","# net parameters\n","n_epochs = 20\n","batch_sz = 2\n","sd=1\n","\n","# Checkpoint definition\n","csv_logger = CSVLogger('./log.out', append=True, separator=';')\n","earlystopping = EarlyStopping(monitor = 'val_binary_accuracy',verbose = 1, min_delta = 0.001, patience = 3, mode = 'max', restore_best_weights = True)\n","callbacks_list = [csv_logger, earlystopping]\n","\n","# Train model\n","results = model.fit_generator(XYaugmentGenerator(X_train,Y_train, batch_sz, sd), \n","                              steps_per_epoch = np.ceil(float(len(X_train))/float(batch_sz)),\n","                              validation_data = val_datagen.flow(X_val,Y_val,batch_sz), \n","                              validation_steps = np.ceil(float(len(X_val))/float(batch_sz)),\n","                              shuffle = True,\n","                              epochs = n_epochs,\n","                              callbacks = callbacks_list) "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2ouhA01nlaM0"},"outputs":[],"source":["#Saving trained model\n","\n","current_net = 'x1'\n","if not os.path.exists('drive/MyDrive/gruppo_FA_DO_PA_PA/_TRAINED_MODELS/'):\n","    os.mkdir('drive/MyDrive/gruppo_FA_DO_PA_PA/_TRAINED_MODELS/')\n","\n","model.save('drive/MyDrive/gruppo_FA_DO_PA_PA/_TRAINED_MODELS/' + current_net)\n"]}],"metadata":{"colab":{"collapsed_sections":[],"name":"script_di_training.ipynb","provenance":[{"file_id":"1AqznFaVaIM13ETF56undEz5G0EGArHI6","timestamp":1636381508060},{"file_id":"15u0PXxVE3F5Qa5B3eKEJ2Rkf4Z1i0Q8V","timestamp":1634720935124}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}